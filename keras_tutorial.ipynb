{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Develop RNN Models with Keras\n",
    "In this tutorial we go over creating two keras models. \n",
    "\n",
    "The first model is a simple Artificial Neural Network (ANN) with Keras. \n",
    "\n",
    "The second model is RNN model based on LSTM architeture. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jupyter Environment\n",
    "Let's start by getting familiar with jupyter environment and some simple tricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell for some simple commands.\n",
    "# Press ctrl+enter to execute a cell\n",
    "# Use shift+enter to execute a cell and move on to the next cell\n",
    "a = 1\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Simple ANN model with Keras\n",
    "\n",
    "### 1. Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# import keras layers needed\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Import keras Sequential model\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Import Scikit-learn data splitting functions\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Download a sample dataset\n",
    "Then we will download a sample data set. The dataset we will be using is \"Appliances Energy Prediction Dataset\".\n",
    "\n",
    "Here is more information about his data set.\n",
    "https://archive.ics.uci.edu/ml/datasets/Appliances+energy+prediction\n",
    "\n",
    "Attribute Information:\n",
    "\n",
    "date time year-month-day hour:minute:second<br>\n",
    "Appliances, energy use in Wh<br>\n",
    "lights, energy use of light fixtures in the house in Wh<br>\n",
    "T1, Temperature in kitchen area, in Celsius<br>\n",
    "RH_1, Humidity in kitchen area, in %<br>\n",
    "T2, Temperature in living room area, in Celsius<br>\n",
    "RH_2, Humidity in living room area, in %<br>\n",
    "T3, Temperature in laundry room area<br>\n",
    "RH_3, Humidity in laundry room area, in %<br>\n",
    "T4, Temperature in office room, in Celsius<br>\n",
    "RH_4, Humidity in office room, in %<br>\n",
    "T5, Temperature in bathroom, in Celsius<br>\n",
    "RH_5, Humidity in bathroom, in %<br>\n",
    "T6, Temperature outside the building (north side), in Celsius<br>\n",
    "RH_6, Humidity outside the building (north side), in %<br>\n",
    "T7, Temperature in ironing room , in Celsius<br>\n",
    "RH_7, Humidity in ironing room, in %<br>\n",
    "T8, Temperature in teenager room 2, in Celsius<br>\n",
    "RH_8, Humidity in teenager room 2, in %<br>\n",
    "T9, Temperature in parents room, in Celsius<br>\n",
    "RH_9, Humidity in parents room, in %<br>\n",
    "To, Temperature outside (from Chievres weather station), in Celsius<br>\n",
    "Pressure (from Chievres weather station), in mm Hg<br>\n",
    "RH_out, Humidity outside (from Chievres weather station), in %<br>\n",
    "Wind speed (from Chievres weather station), in m/s<br>\n",
    "Visibility (from Chievres weather station), in km<br>\n",
    "Tdewpoint (from Chievres weather station), Â°C<br>\n",
    "rv1, Random variable 1, nondimensional<br>\n",
    "rv2, Random variable 2, nondimensional<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get a sample dataset as a pandas dataframe\n",
    "df = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00374/energydata_complete.csv\")\n",
    "\n",
    "\n",
    "# Print a few row of the data\n",
    "df..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many samples do we have in this data set?\n",
    "print(\"Total number of samples: \", df.shape)\n",
    "\n",
    "# Let's visualize some of the data\n",
    "\n",
    "n_samples = 1000\n",
    "feature_name = \"T7\"\n",
    "target_name = \"T2\"\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "ax2 = ax1.twinx()\n",
    "ax1.plot(df[feature_name].values[:n_samples], 'b-')\n",
    "ax2.plot(df[target_name].values[:n_samples], 'g-')\n",
    "ax1.set_xlabel('Samples')\n",
    "ax1.set_ylabel(feature_name, color='b')\n",
    "ax2.set_ylabel(target_name, color='g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create input and output\n",
    "We should extract our inputs and outputs from the dataframe. We will use the living room temperature as the target. To further speed up training, we will use a subset of all the available features.\n",
    "We will also exclude the temperature of the kitchen as it is very correlated with the living room temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_use = [\"lights\", # energy use of light fixtures in the house in Wh\n",
    "                   \"T3\", # Temperature in laundry room area\n",
    "                   \"T4\", # Temperature in office room, in Celsius\n",
    "                   \"T5\", # Temperature in bathroom, in Celsius\n",
    "                   \"T6\", # Temperature outside the building (north side), in Celsius\n",
    "                   \"T7\", # Temperature in ironing room , in Celsius\n",
    "                   \"T8\", # Temperature in teenager room 2, in Celsius\n",
    "                   \"T9\", # Temperature in parents room, in Celsius\n",
    "                   \"T_out\", # Temperature outside (from Chievres weather station), in Celsius\n",
    "                   \"Press_mm_hg\", # (from Chievres weather station), in mm Hg\n",
    "                   \"RH_out\", # Humidity outside (from Chievres weather station), in %\n",
    "                   \"Windspeed\", # Windspeed (from Chievres weather station), in m/s\n",
    "                   \"Visibility\", # Visibility (from Chievres weather station), in km\n",
    "                   \"Tdewpoint\" # Dew point (from Chievres weather station), Â°C\n",
    "                  ]\n",
    "# Grab a portion of the data to make training and testing faster\n",
    "samples = -1\n",
    "stride = 3\n",
    "data = df[features_to_use].values[::stride, :]\n",
    "target = df[target_name].values[::stride].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Split the data into train, test, validation\n",
    "For training a model and evaluating the performance, we devide the model into train, validation, and test sets. \n",
    "\n",
    "We will use the training and validation set to design the architecture, train the model, and optimize the hyperparameters. Then use the test set to report the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine train test splits\n",
    "test_ratio = 0.15\n",
    "\n",
    "# Split the data into training and testing\n",
    "x_trn, x_tst, y_trn, y_tst = train_test_split(data, target, test_size=test_ratio, shuffle=True, random_state=0)\n",
    "\n",
    "# Split the training data into training and validation\n",
    "x_trn, x_vld, y_trn, y_vld = train_test_split(x_trn, y_trn, test_size=test_ratio, shuffle=True, random_state=0)\n",
    "\n",
    "# Print how many samples we have in each set\n",
    "print(\"Number of samples in the training set: \", ...)\n",
    "print(\"Number of samples in the validation set: \", ...)\n",
    "print(\"Number of samples in the test set: \", ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Normalize the Data\n",
    "Most of the time, we should \"prepare\" our data and make it ready for model development. The preperation might include dealing with missing data, normalization, etc. \n",
    "\n",
    "Here, we will normalize the data. Can you explain why we need to normalize the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "mean = x_trn.mean(axis=0)\n",
    "std = x_trn.std(axis=0)\n",
    "x_trn = ...\n",
    "x_vld = ...\n",
    "x_tst = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Developing ANN Model\n",
    "First, we need to decide on the architecture. For now, we can start by a two layer ANN, where the activation function of the first layer can be \"relu\" or \"sigmoid\" (Feel free to try both). The activation funciton of the last layer should be linear as our task is regression.  \n",
    "\n",
    "There are two ways of developing a keras model, Sequential, and Functional. In a nutshell, in sequential models, we keep stacking layers on top of each other until we get the desired architecture. \n",
    "\n",
    "Funcitonal models give more flexibility is desinging complext architectues with multiple inputs, multiple outputs, etc. They start by first determining the inputs and then passing them through the desired blocks.  \n",
    "\n",
    "Let's start by creating a sequential model to predict housing prices in Boston!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we create a placeholder for the model\n",
    "model = ...\n",
    "\n",
    "# Then, we add the two layers\n",
    "model.add(Dense(units=10, activation=..., input_shape=x_trn.shape[1:]))\n",
    "model.add(Dense(units=1, activation=...))\n",
    "\n",
    "# Now, we can compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mae')\n",
    "\n",
    "# And see the summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Training the model\n",
    "We can now start training the model. Once this model is trained, we can go back and add more layers to make it more powerful. We should be careful about overfitting though. That's why we should keep an eye on the validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model, history will include some informaiton about the training...\n",
    "history = model.fit(...,\n",
    "                    ...,\n",
    "                    validation_data=(..., ...),\n",
    "                    epochs=50,\n",
    "                    batch_size=32)\n",
    "\n",
    "# Plot the training and validation loss curves\n",
    "plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Making Predictions with the Trained Model\n",
    "Once the model is trained and we have optimized the parameteres, we can use it to evalute the performance on the test set or any new input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's evaluate the model on the test set\n",
    "test_loss = model.evaluate(..., ...)\n",
    "print(\"Test accuracy is: \", str(test_loss))\n",
    "\n",
    "# Making prediciton on a new data sample\n",
    "target_prd = model.predict((data-mean)/std)\n",
    "samples_to_plot = 200\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(target[-samples_to_plot:])\n",
    "plt.plot(target_prd[-samples_to_plot:], \"--\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Simple RNN Model with Keras\n",
    "Now that we are familiar with making models in Keras, we can start developing RNN models. The steps are pretty much similar. We should just pick the right layers. In this tutorial we will use LSTM layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Necessary Packages\n",
    "In addition to the packages we loaded before, let's import the following packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LSTM layer from keras\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load the Dataset\n",
    "Let's load the dataset we will be using and print some informaiton about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chop_data(data, target, history_to_use):\n",
    "    k = history_to_use\n",
    "    data_list = []\n",
    "    target_list = []\n",
    "    while k <= data.shape[0]:\n",
    "        data_list += [data[k-history_to_use:k, :]]\n",
    "        target_list += [target[k-history_to_use:k]]        \n",
    "        k += 1\n",
    "    chopped_data = np.stack(data_list, axis=0)\n",
    "    chopped_target = np.stack(target_list)\n",
    "    return chopped_data, chopped_target\n",
    "data_ts, target_ts = chop_data(data, target, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_ts.shape)\n",
    "print(target_ts.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Split the data into train, test, validation\n",
    "This is pretty much the same as before. However, splitting time-series data can be tricky spcecially if the data distribution changes over time. So, shuffling the data before splitting can result in false high accuracy values. For now, we will not get into the details, but something you should read on further. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine train test splits\n",
    "test_ratio = 0.15\n",
    "\n",
    "# Split the data into training and testing\n",
    "x_trn, x_tst, y_trn, y_tst = train_test_split(data_ts, target_ts, test_size=test_ratio, shuffle=True, random_state=0)\n",
    "\n",
    "# Split the training data into training and validation\n",
    "x_trn, x_vld, y_trn, y_vld = train_test_split(x_trn, y_trn, test_size=test_ratio, shuffle=True, random_state=0)\n",
    "\n",
    "# Print how many samples we have in each set\n",
    "print(\"Number of samples in the training set: \", x_trn.shape[0])\n",
    "print(\"Number of samples in the validation set: \", x_vld.shape[0])\n",
    "print(\"Number of samples in the test set: \", x_tst.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Normalize the Data\n",
    "Most of the time, we should \"prepare\" our data and make it ready for model development. The preperation might include dealing with missing data, normalization, etc. \n",
    "\n",
    "Here, we will normalize the data. Can you explain why we need to normalize the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = x_trn.mean(axis=(0, 1))\n",
    "std = x_trn.std(axis=(0, 1))\n",
    "\n",
    "x_trn = (x_trn - mean)/std\n",
    "x_vld = (x_vld - mean)/std\n",
    "x_tst = (x_tst - mean)/std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Developing RNN Model\n",
    "We can use the same Sequential API as before to develop our RNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import initializers\n",
    "# First, we create a placeholder for the model\n",
    "model = ...\n",
    "\n",
    "# Then, we add the two layers\n",
    "model.add(GRU(units=15, return_sequences=..., input_shape=x_trn.shape[1:]))\n",
    "model.add(Dense(units=1, bias_initializer=initializers.Constant(target.mean())))\n",
    "\n",
    "# Now, we can compile the model\n",
    "model.compile(optimizer='Adam',\n",
    "              loss='mae')\n",
    "\n",
    "# And see the summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Training the model\n",
    "We can now start training the model. Once this model is trained, we can go back and add more layers to make it more powerful. We should be careful about overfitting though. That's why we should keep an eye on the validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model, history will include some informaiton about the training...\n",
    "history = model.fit(...,\n",
    "                    ...,\n",
    "                    validation_data=(..., ...),\n",
    "                    epochs=50,\n",
    "                    batch_size=32)\n",
    "\n",
    "# Plot the training and validation loss curves\n",
    "plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's evaluate the model on the test set\n",
    "test_loss = model.evaluate(..., ...)\n",
    "print(\"Test accuracy is: \", str(test_loss))\n",
    "\n",
    "# Make some predictions and plot some of the results\n",
    "target_ts_prd = model.predict((data_ts-mean)/std)\n",
    "samples_to_plot = 200\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(target_ts[-samples_to_plot:, 10])\n",
    "plt.plot(target_ts_prd[-samples_to_plot:, 10], \"--\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What to try next\n",
    "You can read about the following topics if you like to further pursue this topic:\n",
    "- Setting shuffle to False when splitting the data (Why do we get different result? Should we shuffle or not shuffle the data?)\n",
    "- Dropout\n",
    "- BatchNotmalization\n",
    "- Bidirectional RNN\n",
    "- Loss functions (mae, mse, hinge loss, etc.)\n",
    "- Optimization methods (Adam, SGD, Adadelta, etc.)\n",
    "- Regularization\n",
    "- Early stopping\n",
    "\n",
    "Then, revise the model and check if it results in better or worse performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
